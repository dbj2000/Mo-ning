{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-10-1 Convolution.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJn79S67tndH9EQ3711/4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbj2000/Mo-ning/blob/master/Lab-10-1%20Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niKjlpoIk1Tx",
        "colab_type": "text"
      },
      "source": [
        "#**Lab-10-1 Convolution**\n",
        "\n",
        "## **Convolutional Neural Networks**\n",
        "*   이미지 위에서 stride 값 만큼 filter(kernel)을 이동시키면서 겹쳐지는 부분의 각 원소의 값을 곱해서 모두 더한 값을 출력으로 하는 연산\n",
        "\n",
        "앞서 우리는 Softmax Classifier, Neural Networks(MLP) 등의 모델을 통하여 MNIST dataset의 classification 문제를 풀어보았다. 지금까지는 28×28 크기의 이미지를 다루기 위하여 28×28 행렬을 1×784 형태의 벡터로 reshape 해주어 input으로 사용했다. 하지만 이러한 방식은 데이터의 **공간 정보를 유실**시키는 결과를 가져오고, 이로인하여 모델의 성능은 저하될 수밖에 없다.\n",
        "\n",
        "반면, CNN에서는 **Convolution layer** 을 도입하여 이 문제를 해결하게 된다. CNN은 28×28을 일렬로 펼친 형태의 벡터로 reshape해주지 않고, 28×28의 이미지 데이터를 그대로 사용한다.\n",
        "\n",
        "## **Convolution Layer**\n",
        "\n",
        "![](https://kjhov195.github.io/post_img/200110/no_padding_no_strides.gif)\n",
        "\n",
        "위 그림은 4×4의 input data에 3×3의 filter를 적용하여 2×2의 output을 구하는 과정을 보여준다. 이러한 방식으로 Filter를 사용하여 연산해주는 layer를 Convolution Layer라고 한다.\n",
        "\n",
        "## **Stride and Padding**\n",
        "\n",
        "*   **stride** : filter를 한번에 얼마나 이동 할 것인가\n",
        "\n",
        "위 예시에서는 filter가 두 칸씩 이동하고 있으므로, Stride=2 로 설정해준 것이다. default값는 Stride=1이다.\n",
        "\n",
        "*   **padding** : zero-padding\n",
        "\n",
        "![](https://kjhov195.github.io/post_img/200110/full_padding_no_strides.gif)\n",
        "\n",
        "zero padding 은 input의 size를 유지해주면서, edge의 정보를 잃지 않게 하기위하여 사용하는 방법이다. 위 예시와 같이 data의 edge 바깥 부분을 0으로 채워주는 방법을 zero padding이라고 한다. 위와 같이 0으로 두 겹을 쌓아줄 경우 padding=2 옵션이며, 따로 설정해주지 않을 경우에는 padding=0 이 default 옵션으로 사용된다.\n",
        "\n",
        "## **torch.nn.Conv2d()**\n",
        "\n",
        "![](https://kjhov195.github.io/post_img/200110/image3.png)\n",
        "\n",
        "\n",
        "![](https://kjhov195.github.io/post_img/200110/image2.png)\n",
        "\n",
        "ex) 입력채널 1/ 출력채널 1/ 커널 크기 3x3\n",
        "conv = nn.Conv2d(1,1,3)\n",
        "\n",
        "여기서 3x3의 필터가 만들어지고, 이 필터를 통과할 수 있는 output을 뽑고싶다면 out=conv(inputs)라고 표현할 수 있고 input data로 사용하는 데이터는 **torch.Tensor** 여야 하며, 다음과 같은 shape을 가지고 있어야 한다.\n",
        "\n",
        "\n",
        "$$input shape: (N×C×H×W)\n",
        "               (batchsize, channel, height, width)$$\n",
        "\n",
        "\n",
        "## **Convolution Layer: output size**\n",
        "결과적으로, Convolution Layer를 거쳐 나오는 output의 shape은 다음과 같다.\n",
        "\n",
        "$$ output size ={{input size−filter size+(2×padding)}\\over{stride}}+1$$\n",
        "\n",
        "만약 input size가 정사각이 아닌 32x64 와 같은 직사각이라면 (32, 64)의 꼴로 대입해서 계산하면 된다.\n",
        "\n",
        "## **Neuron과 Convolution**\n",
        "\n",
        "*   perceptron과 Convolution\n",
        "filter의 값들이 Perceptron의 weight값으로 들어간다. 이때 filter도 bias를 가질 수 있다.\n",
        "\n",
        "## **Pooling**\n",
        "\n",
        "CNN에서 사용되는 또 하나의 layer가 있는데, 바로 Pooling layer이다. Pooling layer는 Down sampling을 위해 사용되는데, 대표적인 예시로 Max pooling이나 Average Pooling이 있다.\n",
        "\n",
        "![](https://kjhov195.github.io/post_img/200110/image4.png)\n",
        "Max pooling의 경우, filter 안에 들어오는 elements 중에서 가장 큰 값을 선택하여 output 행렬의 element로 사용하는 방식이다. \n",
        "\n",
        "![](https://kjhov195.github.io/post_img/200110/image5.png)\n",
        "반면, Average pooling의 경우, filter 안에 들어오는 elements에 대하여 평균 값을 구하여 output 행렬의 element로 사용하는 방식이다. \n",
        "\n",
        "## **torch.nn.MaxPool2d()**\n",
        "![](https://kjhov195.github.io/post_img/200110/image6.png)\n",
        "\n",
        "## **CNN implementation**\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fcii8GJ%2FbtqCz5QYoEP%2FWr3y29cpFVN7km6FmUidTK%2Fimg.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi5FvBwlh6no",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b9c30cbc-bac2-41f3-c2b6-69fc7e2b9d5a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "inputs = torch.Tensor(1, 1, 28, 28) # batchsize,channel,height,width\n",
        "conv1 = nn.Conv2d(1, 5, 5) # input channel, output channel, filter size\n",
        "pool = nn.MaxPool2d(2) # pooling의 vector size 2x2\n",
        "out = conv1(inputs)\n",
        "print(out.size())\n",
        "out2 = pool(out)\n",
        "print(out2.size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 24, 24])\n",
            "torch.Size([1, 5, 12, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}